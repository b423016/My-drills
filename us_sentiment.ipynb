{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd8ab18b-8d80-4387-ba0b-9dbdc066ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21aff6c2-ec4b-4c0e-85af-55a5db3b6d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv(r'C:\\Users\\ayush\\Downloads\\0000000000002747_training_twitter_x_y_train (1).csv')\n",
    "test_df=pd.read_csv(r'C:\\Users\\ayush\\Downloads\\0000000000002747_test_twitter_x_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed61020a-3ad9-45f0-bda5-74f7376187e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>567900433542488064</td>\n",
       "      <td>negative</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ColeyGirouard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@SouthwestAir I am scheduled for the morning, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-17 20:16:29 -0800</td>\n",
       "      <td>Washington D.C.</td>\n",
       "      <td>Atlantic Time (Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>569989168903819264</td>\n",
       "      <td>positive</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WalterFaddoul</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@SouthwestAir seeing your workers time in and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-23 14:36:22 -0800</td>\n",
       "      <td>Indianapolis, Indiana; USA</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>568089179520954368</td>\n",
       "      <td>positive</td>\n",
       "      <td>United</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LocalKyle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@united Flew ORD to Miami and back and  had gr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-18 08:46:29 -0800</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment    airline airline_sentiment_gold  \\\n",
       "0  567900433542488064          negative  Southwest                    NaN   \n",
       "1  569989168903819264          positive  Southwest                    NaN   \n",
       "2  568089179520954368          positive     United                    NaN   \n",
       "\n",
       "            name negativereason_gold  retweet_count  \\\n",
       "0  ColeyGirouard                 NaN              0   \n",
       "1  WalterFaddoul                 NaN              0   \n",
       "2      LocalKyle                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0  @SouthwestAir I am scheduled for the morning, ...         NaN   \n",
       "1  @SouthwestAir seeing your workers time in and ...         NaN   \n",
       "2  @united Flew ORD to Miami and back and  had gr...         NaN   \n",
       "\n",
       "               tweet_created              tweet_location  \\\n",
       "0  2015-02-17 20:16:29 -0800             Washington D.C.   \n",
       "1  2015-02-23 14:36:22 -0800  Indianapolis, Indiana; USA   \n",
       "2  2015-02-18 08:46:29 -0800                    Illinois   \n",
       "\n",
       "                user_timezone  \n",
       "0      Atlantic Time (Canada)  \n",
       "1  Central Time (US & Canada)  \n",
       "2  Central Time (US & Canada)  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09b2d592-2980-4041-bec2-d2ff6d75deac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>569682010270101504</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zsalim03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir In car gng to DFW. Pulled over 1h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 18:15:50 -0800</td>\n",
       "      <td>Texas</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>569608307184242688</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sa_craig</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir after all, the plane didn’t land ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 13:22:57 -0800</td>\n",
       "      <td>College Station, TX</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>567879304593408001</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DanaChristos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>@SouthwestAir can't believe how many paying cu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-17 18:52:31 -0800</td>\n",
       "      <td>CT</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id    airline airline_sentiment_gold          name  \\\n",
       "0  569682010270101504   American                    NaN      zsalim03   \n",
       "1  569608307184242688   American                    NaN      sa_craig   \n",
       "2  567879304593408001  Southwest                    NaN  DanaChristos   \n",
       "\n",
       "  negativereason_gold  retweet_count  \\\n",
       "0                 NaN              0   \n",
       "1                 NaN              0   \n",
       "2                 NaN              1   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0  @AmericanAir In car gng to DFW. Pulled over 1h...         NaN   \n",
       "1  @AmericanAir after all, the plane didn’t land ...         NaN   \n",
       "2  @SouthwestAir can't believe how many paying cu...         NaN   \n",
       "\n",
       "               tweet_created       tweet_location               user_timezone  \n",
       "0  2015-02-22 18:15:50 -0800                Texas  Central Time (US & Canada)  \n",
       "1  2015-02-22 13:22:57 -0800  College Station, TX  Central Time (US & Canada)  \n",
       "2  2015-02-17 18:52:31 -0800                   CT  Eastern Time (US & Canada)  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "803395c4-074a-4718-a8a1-88f4f9dec3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ad7aba3-c5bb-4e50-a849-7e887318b295",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train_df['text'] # will contain the text data (i.e., the actual tweet content)\n",
    "y_train=train_df['airline_sentiment'] # contain the target labels or sentiment classes (e.g., positive, neutral, negative)\n",
    "X_test=test_df['text'] # contain the text data that is being used to test the sentiment analysis model after it has been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b3a530b-fb72-40d6-9874-1577d1e9a9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_y_train_documents = []\n",
    "for i in range(len(X_train)):\n",
    "    document = word_tokenize(X_train[i])\n",
    "    category = y_train[i]\n",
    "    x_y_train_documents.append((document, category))\n",
    "    # x_y_train_documents: After the loop finishes, x_y_train_documents will be a list where each entry is a tuple of the form (document, category).\n",
    "# Example entry: (['this', 'flight', 'is', 'great', '!'], 'positive'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4b4a811-5358-4ddc-8b9d-e3afe94478cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_documents = []\n",
    "for i in range(len(X_test)):\n",
    "    document = word_tokenize(X_test[i])\n",
    "    x_test_documents.append((document))\n",
    "    # same as above for teets split data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2844bc9-f6b8-4f1d-96e4-9c2a0f635ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['@', 'SouthwestAir', 'I', 'am', 'scheduled', 'for', 'the', 'morning', ',', '2', 'days', 'after', 'the', 'fact', ',', 'yes', '..', 'not', 'sure', 'why', 'my', 'evening', 'flight', 'was', 'the', 'only', 'one', 'Cancelled', 'Flightled'], 'negative')\n"
     ]
    }
   ],
   "source": [
    "print(x_y_train_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a68a2f98-0848-4121-bc4e-4b95e3ac057b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@', 'AmericanAir', 'In', 'car', 'gng', 'to', 'DFW', '.', 'Pulled', 'over', '1hr', 'ago', '-', 'very', 'icy', 'roads', '.', 'On-hold', 'with', 'AA', 'since', '1hr', '.', 'Ca', \"n't\", 'reach', 'arpt', 'for', 'AA2450', '.', 'Wat', '2', 'do', '?']\n"
     ]
    }
   ],
   "source": [
    "print(x_test_documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3df39e5-0928-4cc4-b339-436308dd3857",
   "metadata": {},
   "source": [
    "When you run this code, it will display the first tuple, which contains:\n",
    "\n",
    "document: A list of tokens (words) from the first tweet in the training data.\n",
    "category: The sentiment label corresponding to that tweet.\n",
    "For example, if the first tweet in your training data is \"I love flying with this airline!\" and its sentiment label is 'positive', the output might look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b24f6a39-7369-40a8-b9b4-d99bc1dd7472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "import string\n",
    "from nltk import pos_tag\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b08b4432-f489-4579-b31c-263810b79641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_pos(tag):\n",
    "    if tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith(\"v\"):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b13cbd5-de80-4447-8550-7e195e01e581",
   "metadata": {},
   "source": [
    "the get_simple_pos function is designed to convert Part-Of-Speech (POS) tags from the Penn Treebank notation (used by the nltk library) to a simpler set of POS tags that are used by WordNet, a lexical database for the English language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "41f1186c-302a-46cd-b40d-6cad0940825e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "db75ed7b-40ce-483c-a8b2-1b2e3c541518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('better', 'RBR')]\n",
      "r\n"
     ]
    }
   ],
   "source": [
    "print(pos_tag([\"better\"]))\n",
    "print(get_simple_pos(\"R\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f5798807-b753-46c5-8e36-4c6114448be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "stop = stopwords.words(\"english\")\n",
    "punctuations = list(string.punctuation)\n",
    "stop = stop + punctuations\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849d3fbf-75f2-4b7b-be68-564f15a05ae2",
   "metadata": {},
   "source": [
    "The combined list of stop words and punctuation is typically used in text preprocessing for NLP tasks. Here's why each component is important:\n",
    "\n",
    "Stop Words: These are common words that usually do not contribute to the semantic meaning of a text and are often removed to reduce noise in text analysis.\n",
    "Punctuation: Punctuation marks are generally not useful for tasks like sentiment analysis, topic modeling, or other forms of text classification and are often removed to simplify the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dff5cce1-c7d7-4b4f-8eaf-3f6baebb08bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review(words):     # array -> words\n",
    "    output_words = []\n",
    "    for w in words:\n",
    "        if w.lower() not in stop:\n",
    "            # lemmatize word\n",
    "            pos = pos_tag([w])\n",
    "            clean_word = lemmatizer.lemmatize(w, get_simple_pos(pos[0][1]))\n",
    "            output_words.append(clean_word)\n",
    "    return output_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cdcdc2-9252-4d74-89d9-248a8dba4fc3",
   "metadata": {},
   "source": [
    "Stop Word Removal: Removes common words that do not add much semantic meaning (e.g., \"the\", \"is\", \"and\") to reduce noise in the data.\n",
    "Case Normalization: Converts all words to lowercase to ensure that words like \"The\" and \"the\" are treated the same.\n",
    "Lemmatization: Reduces words to their base form to normalize words like \"running\", \"ran\", and \"runs\" to \"run\". This helps in grouping words with similar meanings and improves the quality of text features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a9507776-08b0-4f8d-bc5b-9915a57e6bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_documents = [(clean_review(document),category) for document,category in x_y_train_documents]\n",
    "testing_documents = [(clean_review(document)) for document in X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6dd160-e326-4cd5-8100-8ac6e477393b",
   "metadata": {},
   "source": [
    "training_documents: Prepares the training data by cleaning each document and pairing it with its corresponding category label. This cleaned data can be used to train a machine learning model.\n",
    "\n",
    "testing_documents: Prepares the test data by cleaning each document in a similar manner to the training data, but without any associated category labels. This cleaned test data can be used to evaluate the trained model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "aadb6e03-54c8-479e-bb82-97cb4da8241f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['SouthwestAir', 'scheduled', 'morning', '2', 'day', 'fact', 'yes', '..', 'sure', 'evening', 'flight', 'one', 'Cancelled', 'Flightled'], 'negative'), (['SouthwestAir', 'seeing', 'worker', 'time', 'time', 'going', 'beyond', 'love', 'flying', 'guy', 'Thank'], 'positive')]\n"
     ]
    }
   ],
   "source": [
    "print(training_documents[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "28fec4bc-97b5-453a-8d2d-84df55530909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['e', 'r', 'c', 'n', 'r', ' ', 'n', ' ', 'c', 'r', ' ', 'g', 'n', 'g', ' ', ' ', 'F', 'W', ' ', 'P', 'u', 'l', 'l', 'e', ' ', 'v', 'e', 'r', ' ', '1', 'h', 'r', ' ', 'g', ' ', ' ', 'v', 'e', 'r', ' ', 'c', ' ', 'r', ' ', 'n', 'h', 'l', ' ', 'w', 'h', ' ', ' ', 'n', 'c', 'e', ' ', '1', 'h', 'r', ' ', 'C', 'n', ' ', 'r', 'e', 'c', 'h', ' ', 'r', 'p', ' ', 'f', 'r', ' ', '2', '4', '5', '0', ' ', 'W', ' ', '2', ' '], ['e', 'r', 'c', 'n', 'r', ' ', 'f', 'e', 'r', ' ', 'l', 'l', ' ', 'h', 'e', ' ', 'p', 'l', 'n', 'e', ' ', 'n', '’', ' ', 'l', 'n', ' ', 'n', ' ', 'e', 'n', 'c', 'l', ' ', 'r', ' ', 'w', 'r', 'e', ' ', 'c', 'n', 'n', ' ', ' ', 'G', 'R', 'K', ' ', 'c', 'c', 'r', 'n', 'g', ' ', ' ', 'E', 'R']]\n"
     ]
    }
   ],
   "source": [
    "print(testing_documents[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4984c5dd-51fb-4d02-a4ad-26b2c94ac89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create feature\n",
    "all_words = []\n",
    "for doc in training_documents:\n",
    "    all_words += doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "28fcf03d-301a-4501-b3b9-f3c56f290bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120030"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9831563a-7e29-47d6-9343-de85acc74eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('united', 2928),\n",
       " ('flight', 2806),\n",
       " ('AmericanAir', 2208),\n",
       " ('USAirways', 2173),\n",
       " ('SouthwestAir', 1801)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = nltk.FreqDist(all_words)\n",
    "common = freq.most_common(5000)\n",
    "common[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb849a2-84b8-47a6-8167-3fe1b4f8e6e1",
   "metadata": {},
   "source": [
    "common = freq.most_common(5000):\n",
    "\n",
    "This method returns a list of the 5,000 most common words and their frequencies from the frequency distribution freq.\n",
    "The most_common(5000) function returns the words in descending order of frequency, meaning the most frequent word will be first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "75addad3-cfd0-4cd1-a052-ce6a1b0758ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [i[0] for i in common]\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d4009dfb-a9d1-49a8-b38c-fec194f2986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_data = [\" \".join(document) for document, category in training_documents]\n",
    "y_train_data = [category for document, category in training_documents]\n",
    "y_train_data = np.array(y_train_data)\n",
    "x_test_data = [\" \".join(document) for document in testing_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b1ee387a-70e5-48da-9a72-64c3e01ad2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "united 50 min hold trying book award flight site error Agents ask pin see platinum availability badbadbad\n"
     ]
    }
   ],
   "source": [
    "print(x_train_data[100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a285a0fd-78bd-4dcf-a390-083cdbcf9f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e r c n r   n   c r   g n g     F W   P u l l e   v e r   1 h r   g     v e r   c   r   n h l   w h     n c e   1 h r   C n   r e c h   r p   f r   2 4 5 0   W   2  \n"
     ]
    }
   ],
   "source": [
    "print(x_test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e6ba1db5-8c9f-4ff1-b296-c2a95a272b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "26ce42c6-025c-40de-a284-1abaad5a7b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction\n",
    "count_vec = CountVectorizer(max_features = 10000, ngram_range = (1,2), max_df = 0.7)\n",
    "x_train_features = count_vec.fit_transform(x_train_data)\n",
    "x_test_features = count_vec.transform(x_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1dc17a-962b-4f42-b8ab-e29fffb95c50",
   "metadata": {},
   "source": [
    "max_features=10000: Limits the vocabulary to the top 10,000 most frequent words.\n",
    "ngram_range=(1, 2): Considers unigrams (single words) and bigrams (pairs of words) as features.\n",
    "max_df=0.7: Ignores terms that appear in more than 70% of the documents.\n",
    "fit_transform: Learns the vocabulary from the training data and transforms the text into \n",
    "a numerical representation (sparse matrix) based on word counts.\n",
    "transform: Uses the vocabulary learned from the training data to convert \n",
    "the testing data into a numerical representation (sparse matrix) based on word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d9c96a0b-c2fa-4b90-b4b9-dbd1d81824b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10980, 10000), (10980,), (3660, 10000))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_features.shape, y_train_data.shape, x_test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "69a434c7-36f1-4c7a-98aa-648d130808cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_features.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5951d523-c4c2-4b4a-8baf-cf37b6467619",
   "metadata": {},
   "source": [
    "Sparse matrix: A matrix where most elements are zero. This is often used to efficiently store and manipulate large matrices with many zeros.\n",
    "Dense matrix: A matrix where all elements are stored, even if they are zero.\n",
    "The todense() method is typically used when you need to perform operations that are more efficient on dense matrices, such as:\n",
    "\n",
    "Matrix multiplication: Certain matrix multiplication algorithms are optimized for dense matrices.\n",
    "Element-wise operations: Operations like addition, subtraction, and division can be faster on dense matrices.\n",
    "Visualization: Dense matrices are easier to visualize and understand than sparse matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3fab7f95-3564-44b0-9a7f-6299be191856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'positive', 'positive', ..., 'negative', 'negative',\n",
       "       'negative'], dtype='<U8')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890df6fd-0563-4db5-8988-26f003486c7e",
   "metadata": {},
   "source": [
    "dtype='<U8' in NumPy refers to a Unicode string data type with a fixed length of 8 characters.\n",
    "\n",
    "Breakdown:\n",
    "\n",
    "dtype: This indicates that we're specifying a data type.\n",
    "'<U8':\n",
    "'<' denotes a little-endian byte order.\n",
    "'U' specifies a Unicode string.\n",
    "'8' indicates a fixed length of 8 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a650223e-fd49-4709-b9b6-f84fbdeacbca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '000 foot', ..., 'zkatcher', 'zone', 'zurich'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fe696b07-bbca-45ea-a03c-711080d32533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.916120218579235"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC()\n",
    "svc.fit(x_train_features, y_train_data)\n",
    "svc.score(x_train_features, y_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d34315b4-35c5-4430-970d-1bab5a31c3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9941712204007286"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(x_train_features, y_train_data)\n",
    "rfc.score(x_train_features, y_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ec6563a2-cea6-4f6e-aaec-a6b36840e978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Weighted): 0.9941736308042968\n",
      "Accuracy: 0.9941712204007286\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00      6851\n",
      "     neutral       0.99      0.99      0.99      2327\n",
      "    positive       0.98      0.99      0.99      1802\n",
      "\n",
      "    accuracy                           0.99     10980\n",
      "   macro avg       0.99      0.99      0.99     10980\n",
      "weighted avg       0.99      0.99      0.99     10980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "\n",
    "# Assuming you have already split your data into training and test sets, and x_train_features, y_train_data\n",
    "\n",
    "# Train the Random Forest Classifier\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(x_train_features, y_train_data)\n",
    "\n",
    "# Make predictions on the training data\n",
    "y_pred_train = rfc.predict(x_train_features)\n",
    "\n",
    "# Calculate the F1 score\n",
    "f1 = f1_score(y_train_data, y_pred_train, average='weighted')  # Use 'weighted', 'micro', or 'macro'\n",
    "print(f\"F1 Score (Weighted): {f1}\")\n",
    "\n",
    "# Calculate the accuracy score\n",
    "accuracy = accuracy_score(y_train_data, y_pred_train)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_train_data, y_pred_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252c859b-b3c6-4eb3-b397-28525fb0b8dd",
   "metadata": {},
   "source": [
    "f1_score(y_train_data, y_pred_train, average='weighted'):\n",
    "Calculates the F1 score considering the imbalanced classes by giving different weights to each class based on their support (i.e., number of true instances for each label).\n",
    "classification_report(y_train_data, y_pred_train):\n",
    "Provides a detailed report showing precision, recall, and F1 score for each class.\n",
    "You can change the average parameter to 'micro' or 'macro' depending on how you want to aggregate the F1 scores across classes. If you set average=None, it will return a separate F1 score for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "effa2d47-cc98-46b3-a915-cfd390d4cb74",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m y_pred_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43my_pred_train\u001b[49m)\n\u001b[0;32m      3\u001b[0m y_pred_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred_train' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "y_pred_df = pd.DataFrame(y_pred_train)\n",
    "y_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a88113c-9b9f-4966-ba99-9562345035b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df.to_csv(\"Y_Predicted.csv\", header = False, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
